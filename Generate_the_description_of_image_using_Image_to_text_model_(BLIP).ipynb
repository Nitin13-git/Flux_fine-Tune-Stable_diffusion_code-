{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Automatic Image Captioning with BLIP\n",
        "\n",
        "This notebook automates the process of generating descriptive captions for a collection of images using the BLIP (Bootstrapped Language-Image Pre-training) model from Hugging Face. BLIP is a powerful model that can generate captions for images, and this notebook streamlines the process for an entire directory of images, storing the results in a structured CSV file.\n",
        "Key Components of the Code:\n",
        "\n",
        "    Pre-trained BLIP Model:\n",
        "        The model used for captioning is Salesforce's BLIP-Image-Captioning-Large, a pre-trained model that generates high-quality captions for images.\n",
        "        The model is loaded using Hugging Face's transformers library and is moved to the GPU (CUDA) for faster processing.\n",
        "\n",
        "    Directory-Based Image Captioning:\n",
        "        The script processes all images in a specified input directory (image_directory), generating captions for each image.\n",
        "        It supports common image formats such as .jpg, .jpeg, and .png.\n",
        "\n",
        "    Caption Generation Process:\n",
        "        For each image, the script:\n",
        "            Loads the image and converts it to RGB format.\n",
        "            Feeds the image into the BLIP model to generate a caption.\n",
        "            Decodes the generated caption into text form.\n",
        "\n",
        "    Saving Captions to CSV:\n",
        "        Instead of generating separate text files for each image, the captions are saved in a single CSV file (Metadata.csv).\n",
        "        The CSV file contains two columns: filename (the name of the image) and prompt (the generated caption for the image).\n",
        "        This approach provides a consolidated view of all image captions, making it easier to manage and analyze the data.\n",
        "\n",
        "How to Use:\n",
        "\n",
        "    Input and Output Directory Setup:\n",
        "        Place your images in the specified folder, and the script will process all valid image files in that directory.\n",
        "        The output CSV file (Metadata.csv) will be saved in the specified location, containing filenames and their corresponding captions.\n",
        "\n",
        "Why Use This Script?\n",
        "\n",
        "    Efficiency:\n",
        "        The BLIP model requires significant computational power, particularly for large-scale image processing. Using a GPU accelerates the process, making it ideal for environments like Google Colab, which provides easy access to GPU resources.\n",
        "    Automated Captioning:\n",
        "        The script automates the task of captioning images in bulk, which is useful for tasks such as annotating image datasets, generating content for media, or creating alternative text descriptions for web images.\n",
        "\n",
        "Important Note:\n",
        "\n",
        "    Manual Review of Captions:\n",
        "        While the model generates high-quality captions, it's recommended to manually review the captions to ensure their accuracy. The captions may sometimes require adjustments, depending on the context and specific use cases.\n",
        "\n",
        "#Once the script completes, you'll have a Metadata.csv file that stores all the image filenames and their corresponding captions, streamlining the captioning process and providing a structured output for further use."
      ],
      "metadata": {
        "id": "IBwjFTsj4r2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(\"cuda\")\n",
        "\n",
        "def generate_captions_for_directory(image_dir, output_csv_path):\n",
        "    data = []\n",
        "\n",
        "    for filename in os.listdir(image_dir):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "\n",
        "            image_path = os.path.join(image_dir, filename)\n",
        "            raw_image = Image.open(image_path).convert('RGB')\n",
        "            inputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n",
        "            out = model.generate(**inputs)\n",
        "            caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "            data.append([filename, caption])\n",
        "\n",
        "\n",
        "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        csvwriter = csv.writer(csvfile)\n",
        "        csvwriter.writerow(['filename', 'prompt'])\n",
        "        csvwriter.writerows(data)\n",
        "\n",
        "    print(f\"Metadata saved to {output_csv_path}\")\n",
        "\n",
        "# Define input directory and output CSV file path\n",
        "image_directory = \" \"  # Replace with your image directory path\n",
        "output_csv = \"content/drive/mydrive////Metadata.csv\"  # Output CSV file path(actual path where you want to store)\n",
        "\n",
        "generate_captions_for_directory(image_directory, output_csv)\n"
      ],
      "metadata": {
        "id": "xQwfj6iazN3w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}