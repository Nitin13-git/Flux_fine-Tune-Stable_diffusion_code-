{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Automatic Image Captioning with BLIP\n",
        "\n",
        "This notebook automates the process of generating descriptive captions for a collection of images using the BLIP (Bootstrapped Language-Image Pre-training) model from Hugging Face. BLIP is a powerful model that can generate captions for images, and this notebook makes it easy to use this capability across an entire directory of images.\n",
        "Key Components of the Code:\n",
        "\n",
        "    Pre-trained BLIP Model:\n",
        "        The model used for captioning is Salesforce's BLIP-Image-Captioning-Large, a pre-trained model that generates high-quality captions for images.\n",
        "        The model is loaded using Hugging Face's transformers library and is moved to the GPU (cuda) for faster processing.\n",
        "\n",
        "    Directory-Based Image Captioning:\n",
        "        The script processes all images in a specified input directory (image_directory), generating captions for each image.\n",
        "        It supports common image formats such as .jpg, .jpeg, and .png.\n",
        "\n",
        "    Caption Generation Process:\n",
        "        For each image, the script:\n",
        "            Loads the image and converts it to RGB format.\n",
        "            Feeds the image into the BLIP model to generate a caption.\n",
        "            Decodes the generated caption into text form.\n",
        "\n",
        "    Saving Captions:\n",
        "        Captions are saved in individual text files with the same name as the corresponding image file, but with a .txt extension.\n",
        "        All caption files are stored in the specified output directory (output_directory).\n",
        "\n",
        "How to Use:\n",
        "\n",
        "    Input and Output Directory Setup:\n",
        "        You need to upload your images to a specified folder in Google Drive, which is linked to Colab.\n",
        "        The output directory is where the generated captions (in .txt files) will be saved.\n",
        "\n",
        "    Why Use Colab?\n",
        "        Due to the computational requirements of the BLIP model, especially when using large versions of the model, running this code on a GPU is essential for efficiency. Colab provides easy access to GPU resources and has the necessary Python libraries (like transformers and torch) pre-installed or easy to install.\n",
        "\n",
        "What the Code Does:\n",
        "\n",
        "    This script is designed to automate caption generation for batches of images, making it useful for tasks like image dataset annotation, content creation, or even generating alternative text descriptions for web images.\n",
        "\n",
        "Once the script runs, you'll have a collection of text files, each containing a caption that describes its corresponding image. This process saves time by eliminating the need for manual captioning."
      ],
      "metadata": {
        "id": "IBwjFTsj4r2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# Load BLIP model and processor\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(\"cuda\")\n",
        "\n",
        "def generate_captions_for_directory(image_dir, output_dir):\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(image_dir):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            # Process the image and generate caption\n",
        "            image_path = os.path.join(image_dir, filename)\n",
        "            raw_image = Image.open(image_path).convert('RGB')\n",
        "            inputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n",
        "            out = model.generate(**inputs)\n",
        "            caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "            # Save the caption to a text file\n",
        "            text_file_path = os.path.join(output_dir, os.path.splitext(filename)[0] + \".txt\")\n",
        "            with open(text_file_path, 'w') as f:\n",
        "                f.write(caption)\n",
        "\n",
        "# Define input and output directories\n",
        "image_directory = \" \"\n",
        "output_directory = \" \"\n",
        "\n",
        "generate_captions_for_directory(image_directory, output_directory)\n"
      ],
      "metadata": {
        "id": "ncbuFSve4t2n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}